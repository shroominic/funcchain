{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Funcchain","text":""},{"location":"#introduction","title":"Introduction","text":"<p><code>funcchain</code> is the most pythonic way of writing cognitive systems. Leveraging pydantic models as output schemas combined with langchain in the backend allows for a seamless integration of llms into your apps. It utilizes perfect with OpenAI Functions or LlamaCpp grammars (json-schema-mode) for efficient structured output. In the backend it compiles the funcchain syntax into langchain runnables so you can easily invoke, stream or batch process your pipelines.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>$ pip install funcchain\n---&gt; 100%\n</code></pre> <p>Important</p> <p>Make sure to have an OpenAI API key in your environment variables:</p> <p><pre><code>export OPENAI_API_KEY=\"sk-rnUPxirSQ4bmz2He4qyaiKShdXJcsOsTg\"\n</code></pre> (not needed for local models of course)</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udc0d pythonic</li> <li>\ud83d\udd00 easy swap between openai or local models</li> <li>\ud83d\udd04 dynamic output types (pydantic models, or primitives)</li> <li>\ud83d\udc41\ufe0f vision llm support</li> <li>\ud83e\udde0 langchain_core as backend</li> <li>\ud83d\udcdd jinja templating for prompts</li> <li>\ud83c\udfd7\ufe0f reliable structured output</li> <li>\ud83d\udd01 auto retry parsing</li> <li>\ud83d\udd27 langsmith support</li> <li>\ud83d\udd04 sync, async, streaming, parallel, fallbacks</li> <li>\ud83d\udce6 gguf download from huggingface</li> <li>\u2705 type hints for all functions and mypy support</li> <li>\ud83d\udde3\ufe0f chat router component</li> <li>\ud83e\udde9 composable with langchain LCEL</li> <li>\ud83d\udee0\ufe0f easy error handling</li> <li>\ud83d\udea6 enums and literal support</li> <li>\ud83d\udcd0 custom parsing types</li> </ul>"},{"location":"#usage","title":"Usage","text":"<pre><code>from funcchain import chain\n\ndef hello() -&gt; str:\n    \"\"\"\n    Say hello in 3 languages.\n    \"\"\"\n    return chain()\n\nprint(hello()) # -&gt; \"Hallo, Bonjour, Hola\"\n</code></pre> <p>This will call the OpenAI API and return the response. Its using OpenAI since we did not specify a model and it will use the default model from the global settings of funcchain.</p> <p>The underlying chat will look like this:</p> <ul> <li>User: \"Say hello in 3 languages.\"</li> <li>AI: \"Hallo, Bonjour, Hola\"</li> </ul> <p>The <code>chain()</code> function does all the magic in the background. It extracts the docstring, input arguments and return type of the function and compiles everything into a langchain prompt.</p>"},{"location":"#complex-example","title":"Complex Example","text":"<p>Here a more complex example of what is possible. We create nested pydantic models and use union types to let the model choose the best shape to parse your given list into.</p> <pre><code>from pydantic import BaseModel, Field\nfrom funcchain import chain\n\n# define nested models\nclass Item(BaseModel):\n    name: str = Field(description=\"Name of the item\")\n    description: str = Field(description=\"Description of the item\")\n    keywords: list[str] = Field(description=\"Keywords for the item\")\n\nclass ShoppingList(BaseModel):\n    items: list[Item]\n    store: str = Field(description=\"The store to buy the items from\")\n\nclass TodoList(BaseModel):\n    todos: list[Item]\n    urgency: int = Field(description=\"The urgency of all tasks (1-10)\")\n\n# support for union types\ndef extract_list(user_input: str) -&gt; TodoList | ShoppingList:\n    \"\"\"\n    The user input is either a shopping List or a todo list.\n    \"\"\"\n    return chain()\n\n# the model will choose the output type automatically\nlst = extract_list(\n    input(\"Enter your list: \")\n)\n\n# custom handler based on type\nmatch lst:\n    case ShoppingList(items=items, store=store):\n        print(\"Here is your Shopping List: \")\n        for item in items:\n            print(f\"{item.name}: {item.description}\")\n        print(f\"You need to go to: {store}\")\n\n    case TodoList(todos=todos, urgency=urgency):\n        print(\"Here is your Todo List: \")\n        for item in todos:\n            print(f\"{item.name}: {item.description}\")\n        print(f\"Urgency: {urgency}\")\n</code></pre> <p>The pydantic models force the language model to output only in the specified format. The actual ouput is a json string which is parsed into the pydantic model. This allows for a seamless integration of the language model into your app. The union type selection works by listing every pydantic model as seperate function call to the model. So the LLM will select the best fitting pydantic model based on the prompt and inputs.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#020-2024-02-02","title":"[0.2.0] - 2024-02-02","text":"<p>\ud83d\udd17 langchain lcel compatibility (runnable decorator) \ud83d\udd0d primitive type parsers \ud83d\udd04 dynamic chat routers \ud83d\udcc4 full jinja2 templating \ud83d\udea6 enums, literal output types \ud83e\udd99 llamacpp + ollama support \ud83d\udce1 openai json mode streaming \ud83d\udd01 auto retry for pydantic validations \ud83d\udc41\ufe0f better vision support \u270d\ufe0f create custom parsing types \ud83c\udfd7\ufe0f codebase restructuring \u2b06\ufe0f updated deps</p>"},{"location":"changelog/#0110-2023-12-12","title":"[0.1.10] - 2023-12-12","text":"<p>\ud83d\udd04 universal model loader \ud83d\udee0 improved configuration \u2699\ufe0f SettingOverrides \ud83d\udce5 auto gguf model download \ud83d\udd17 optional dependencies \ud83c\udf1f improve examples \ud83d\udd27 other small improvements</p>"},{"location":"changelog/#010-019-2023-12-01","title":"[0.1.0] - [0.1.9] - 2023-12-01","text":"<p>\ud83c\udd95 pydantic v2 \ud83e\udd99 llamacpp support \ud83d\udd01 auto retry parsing \ud83d\udcc4 jinja templates \u2728 multiple tiny improvements \ud83c\udfd7\ufe0f codebase refactor \ud83d\udc1b bug fixes</p>"},{"location":"changelog/#001-017-2023-10-08","title":"[0.0.1] - [0.1.7] - 2023-10-08","text":"<p>\ud83e\uddea Undocumented Experimental Releases</p>"},{"location":"changelog/#001-2023-08-31","title":"[0.0.1] - 2023-08-31","text":"<p>\ud83c\udf89 Initial Release</p>"},{"location":"advanced/async/","title":"Async","text":""},{"location":"advanced/async/#why-and-how-to-use-using-async","title":"Why and how to use using async?","text":"<p>Asyncronous promgramming is a way to easily parallelize processes in python. This is very useful when dealing with LLMs because every request takes a long time and the python interpreter should do alot of other things in the meantime instead of waiting for the request.</p> <p>Checkout this brillian async tutorial if you never coded in an asyncronous way.</p>"},{"location":"advanced/async/#async-in-funcchain","title":"Async in FuncChain","text":"<p>You can use async in funcchain by creating your functions using <code>achain()</code> instead of the normal <code>chain()</code>. It would then look like this:</p> <pre><code>from funcchain import achain\n\nasync def generate_poem(topic: str) -&gt; str:\n    \"\"\"\n    Generate a poem inspired by the given topic.\n    \"\"\"\n    return await achain()\n</code></pre> <p>You can then <code>await</code> the async <code>generate_poem</code> function inside another async funtion or directly call it using <code>asyncio.run(generate_poem(\"birds\"))</code>.</p>"},{"location":"advanced/async/#async-in-langchain","title":"Async in LangChain","text":"<p>When converting your funcchains into a langchain runnable you can use the native langchain way of async. This would be <code>.ainvoke(...)</code>, <code>.astream(...)</code> or <code>.abatch(...)</code> .</p>"},{"location":"advanced/async/#async-streaming","title":"Async Streaming","text":"<p>You can use langchains async streaming interface but also use the <code>stream_to(...)</code> wrapper (explained here) as an async context manager.</p> <pre><code>async with stream_to(...):\n    await ...\n</code></pre>"},{"location":"advanced/codebase-scaling/","title":"Codebase Scaling","text":""},{"location":"advanced/codebase-scaling/#multi-file-projects","title":"Multi file projects","text":""},{"location":"advanced/codebase-scaling/#todo","title":"TODO","text":""},{"location":"advanced/codebase-scaling/#structure","title":"Structure","text":""},{"location":"advanced/codebase-scaling/#todo_1","title":"TODO","text":""},{"location":"advanced/custom-parser-types/","title":"Custom Parsers","text":""},{"location":"advanced/custom-parser-types/#example","title":"Example","text":""},{"location":"advanced/custom-parser-types/#todo","title":"TODO","text":""},{"location":"advanced/custom-parser-types/#grammars","title":"Grammars","text":""},{"location":"advanced/custom-parser-types/#todo_1","title":"TODO","text":""},{"location":"advanced/custom-parser-types/#format-instructions","title":"Format Instructions","text":""},{"location":"advanced/custom-parser-types/#todo_2","title":"TODO","text":""},{"location":"advanced/custom-parser-types/#parse-function","title":"parse() Function","text":""},{"location":"advanced/custom-parser-types/#todo_3","title":"TODO","text":""},{"location":"advanced/custom-parser-types/#write-your-own-parser","title":"Write your own Parser","text":""},{"location":"advanced/custom-parser-types/#todo_4","title":"TODO","text":""},{"location":"advanced/customization/","title":"Customization","text":""},{"location":"advanced/customization/#extra-args-inside-chain","title":"extra args inside chain","text":""},{"location":"advanced/customization/#todo","title":"TODO","text":""},{"location":"advanced/customization/#low-level-langchain","title":"low level langchain","text":""},{"location":"advanced/customization/#todo_1","title":"TODO","text":""},{"location":"advanced/customization/#extra-args-inside-runnable","title":"extra args inside @runnable","text":""},{"location":"advanced/customization/#todo_2","title":"TODO","text":""},{"location":"advanced/customization/#custom-ll-models","title":"custom ll models","text":""},{"location":"advanced/customization/#todo_3","title":"TODO","text":""},{"location":"advanced/runnables/","title":"runnables","text":""},{"location":"advanced/runnables/#langchain-expression-language-lcel","title":"LangChain Expression Language (LCEL)","text":""},{"location":"advanced/runnables/#todo","title":"TODO","text":""},{"location":"advanced/runnables/#streaming-parallel-async-and","title":"Streaming, Parallel, Async and","text":""},{"location":"advanced/runnables/#todo_1","title":"TODO","text":""},{"location":"advanced/signature/","title":"Signature","text":""},{"location":"advanced/signature/#compilation","title":"Compilation","text":""},{"location":"advanced/signature/#todo","title":"TODO","text":""},{"location":"advanced/signature/#schema","title":"Schema","text":""},{"location":"advanced/signature/#todo_1","title":"TODO","text":""},{"location":"advanced/stream-parsing/","title":"Stream Parsing","text":""},{"location":"advanced/stream-parsing/#transform-output-parsing","title":"Transform Output Parsing","text":""},{"location":"advanced/stream-parsing/#todo","title":"TODO","text":""},{"location":"advanced/stream-parsing/#composing-runnables","title":"Composing Runnables","text":""},{"location":"advanced/stream-parsing/#todo_1","title":"TODO","text":""},{"location":"concepts/chain/","title":"Chain","text":""},{"location":"concepts/chain/#chain_1","title":"<code>chain()</code>","text":"<p>The chain function abstracts away all the magic happening in the funcchain backend. It extracts the docstring, input arguments and return type of the function and compiles everything into a langchain prompt.</p> <pre><code>from funcchain import chain\n\ndef ask(question: str) -&gt; str:\n    \"\"\"\n    Answer the given question.\n    \"\"\"\n    return chain()\n\nask(\"What is the capital of Germany?\")\n# =&gt; \"The capital of Germany is Berlin.\"\n</code></pre>"},{"location":"concepts/chain/#achain","title":"<code>achain()</code>","text":"<p>Async version of the <code>chain()</code> function.</p> <pre><code>import asyncio\nfrom funcchain import achain\n\nasync def ask(question: str) -&gt; str:\n    \"\"\"\n    Answer the given question.\n    \"\"\"\n    return await achain()\n\nasyncio.run(ask(\"What is the capital of Germany?\"))\n# =&gt; \"The capital of Germany is Berlin.\"\n</code></pre>"},{"location":"concepts/chain/#runnable","title":"<code>@runnable</code>","text":"<p>The <code>@runnable</code> decorator is used to compile a chain function into a langchain runnable object. You just write a normal funcchain function using chain() and then decorate it with <code>@runnable</code>.</p> <pre><code>from funcchain import chain, runnable\n\n@runnable\ndef ask(question: str) -&gt; str:\n    \"\"\"\n    Answer the given question.\n    \"\"\"\n    return chain()\n\nask.invoke(input={\"question\": \"What is the capital of Germany?\"})\n# =&gt; \"The capital of Germany is Berlin.\"\n</code></pre>"},{"location":"concepts/errors/","title":"Errors","text":""},{"location":"concepts/errors/#example","title":"Example","text":"<pre><code>from funcchain import BaseModel, Error, chain\nfrom rich import print\n\nclass User(BaseModel):\n    name: str\n    email: str | None\n\ndef extract_user_info(text: str) -&gt; User | Error:\n    \"\"\"\n    Extract the user information from the given text.\n    In case you do not have enough infos, return an error.\n    \"\"\"\n    return chain()\n\nprint(extract_user_info(\"hey what's up?\"))\n# =&gt; Error(title='Invalid Input', description='The input text does not contain user information.')\n\nprint(extract_user_info(\"I'm John and my email is john@mail.com\"))\n# =&gt; User(name='John', email='john@mail.com')\n</code></pre>"},{"location":"concepts/errors/#error-type","title":"Error Type","text":"<p>(currently only supported for union output types e.g. <code>Answer | Error</code> so only openai models)</p> <p>The Error type is a special type that can be used to return an error from a chain function. It is just a pydantic model with a title and description field.</p> <pre><code>class Error(BaseModel):\n    \"\"\"\n    Fallback function for invalid input.\n    If you are unsure on what function to call, use this error function as fallback.\n    This will tell the user that the input is not valid.\n    \"\"\"\n\n    title: str = Field(description=\"CamelCase Name titeling the error\")\n    description: str = Field(..., description=\"Short description of the unexpected situation\")\n\n    def __raise__(self) -&gt; None:\n        raise Exception(self.description)\n</code></pre> <p>You can also create your own error types by inheriting from the Error type. Or just do it similar to the example above.</p>"},{"location":"concepts/input/","title":"Input Arguments","text":"<p>Funcchain utilises your function's input arguments including type hints to compile your prompt. You can utilise the following types:</p>"},{"location":"concepts/input/#strings","title":"Strings","text":"<p>All string inputs serve as classic prompt placeholders and are replaced with the input value. You can insert anything as long as you cast it to a string and the language model will see its as text.</p> <pre><code>def create_username(full_name: str, email: str) -&gt; str:\n    \"\"\"\n    Create a creative username from the given full name and email.\n    \"\"\"\n    return chain()\n</code></pre> <p>All strings that are not mentioned in the instructions are automatically added to the beginning of the prompt.</p> <p>When calling <code>create_username(\"John Doe\", \"john.doe@gmail.com\")</code> the compiled prompt will look like this:</p> <pre><code>&lt;HumanMessage&gt;\n    FULL_NAME:\n    John Doe\n\n    EMAIL:\n    john.doe@gmail.com\n\n    Create a creative username from the given full name and email.\n&lt;/HumanMessage&gt;\n</code></pre> <p>The language model will then be able to use the input values to generate a good username.</p> <p>You can also manually format your instructions if you want to have more control over the prompt. Use jinja2 syntax to insert the input values.</p> <pre><code>def create_username(full_name: str, email: str) -&gt; str:\n    \"\"\"\n    Create a creative username for {{ full_name }} with the mail {{ email }}.\n    \"\"\"\n    return chain()\n</code></pre> <p>Compiles to:</p> <pre><code>&lt;HumanMessage&gt;\n    Create a creative username for John Doe with the mail john.doe@gmail.com.\n&lt;/HumanMessage&gt;\n</code></pre>"},{"location":"concepts/input/#pydantic-models","title":"Pydantic Models","text":"<p>You can also use pydantic models as input arguments. This is useful if you already have complex data structures that you want to use as input.</p> <pre><code>class User(BaseModel):\n    full_name: str\n    email: str\n\ndef create_username(user: User) -&gt; str:\n    \"\"\"\n    Create a creative username from the given user.\n    \"\"\"\n    return chain()\n</code></pre> <p>By default, the pydantic model is converted to a string using the <code>__str__</code> method and then added to the prompt.</p> <pre><code>&lt;HumanMessage&gt;\n    USER:\n    full_name='Herbert Geier' email='hello@bert.com'\n\n    Create a creative username from the given user.\n&lt;/HumanMessage&gt;\n</code></pre> <p>If you want more control you can override the <code>__str__</code> method of your pydantic model. Or use jinja2 syntax to manually unpack the model.</p> <pre><code>class User(BaseModel):\n    full_name: str\n    email: str\n\ndef create_username(user: User) -&gt; str:\n    \"\"\"\n    Create a creative username for {{ user.full_name }} with the mail {{ user.email }}.\n    \"\"\"\n    return chain()\n</code></pre>"},{"location":"concepts/input/#images","title":"Images","text":"<p>todo: write</p>"},{"location":"concepts/input/#other-types","title":"Other Types","text":"<p>More special types are coming soon.</p>"},{"location":"concepts/input/#important-notes","title":"Important Notes","text":"<p>You need to use type hints for all your input arguments. Otherwise, funcchain will just ignore them.</p>"},{"location":"concepts/langchain/","title":"LangChain","text":""},{"location":"concepts/langchain/#what-is-langchain","title":"What is LangChain?","text":"<p>LangChain is the most advanced library for building applications using large language models. Funcchain is built on top of <code>langchain_core</code> which inculdes LangChain Expression Language (LCEL) and alot more powerful core abstractions for building cognitive architectures.</p>"},{"location":"concepts/langchain/#why-building-on-top-of-it","title":"Why building on top of it?","text":"<p>We have been looking into alot of different libraries and wanted to start a llm framework from scratch. But langchain already provides all of the fundamental abstractions we need to use and it is the most advanced library we have found so far.</p>"},{"location":"concepts/langchain/#compatibility","title":"Compatibility","text":"<p>Funcchain is compatible with all langchain chat models, memory stores and runnables. It's using langchain output parsers and the templating system. On the other hand langchain is compatible with funcchain by using the <code>@runnable</code> decorator. This will convert your function into a runnable that can be used in LCEL so you can build your own complex cognitive architectures.</p>"},{"location":"concepts/langchain/#lcel-example-rag","title":"LCEL Example (RAG)","text":"<pre><code>from funcchain import chain, runnable\nfrom langchain_community.vectorstores.faiss import FAISS\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\n@runnable\ndef generate_poem(topic: str, context: str) -&gt; str:\n    \"\"\"\n    Generate a poem about the topic with the given context.\n    \"\"\"\n    return chain()\n\nvectorstore = FAISS.from_texts(\n    [\n        \"cold showers are good for your immune system\",\n        \"i dont like when people are mean to me\",\n        \"japanese tea is full of heart warming flavors\",\n    ],\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n\nretrieval_chain: Runnable = {\n    \"context\": retriever,\n    \"topic\": RunnablePassthrough(),\n} | generate_poem\n\nprint(retrieval_chain.invoke(\"love\"))\n</code></pre> <p>The chain will then retrieve \u00b4japanese tea is full of heart warming flavors` as context since it's the most similar to the topic \"love\".</p> <pre><code># =&gt; In a cup of tea, love's warmth unfurls\n#    Japanese flavors, heartwarming pearls\n#    A sip of love, in every swirl\n#    In Japanese tea, love's essence twirls\n</code></pre> <p>Useful</p> <p>For seeing whats going on inside the LLM you should try the Langsmith integration: Add those lines to .env and funcchain will use langsmith tracing.</p> <pre><code>LANGCHAIN_TRACING_V2=true\nLANGCHAIN_API_KEY=\"ls__api_key\"\nLANGCHAIN_PROJECT=\"PROJECT_NAME\"\n</code></pre> <p>Langsmith is used to understand what happens under the hood of your LLM generations. When multiple LLM calls are used for an output they can be logged for debugging.</p>"},{"location":"concepts/local-models/","title":"Local Models","text":"<p>Funcchain supports local models through the llama.cpp project using the llama_cpp_python bindings.</p>"},{"location":"concepts/local-models/#llamacpp","title":"LlamaCPP","text":"<p>Written in highly optimized C++ code, LlamaCPP is a library for running large language models locally. It uses GGUF files which are a binary format for storing quantized versions of large language models. You can download alot of GGUF models from TheBloke on huggingface.</p>"},{"location":"concepts/local-models/#grammars","title":"Grammars","text":"<p>Context Free Grammars are a powerful abstraction for a deterministic shape of a string. Funcchain utilizes this by forcing local models to respond in a structured way.</p> <p>For example you can create a grammar that forces the model to always respond with a json object. This is useful if you want to use the output of the model in your code.</p> <p>Going one step further you can also create a grammar that forces the model to respond with a specific pydantic model.</p> <p>This is how funcchain is able to use local models in a structured way.</p>"},{"location":"concepts/overview/","title":"Concepts","text":""},{"location":"concepts/overview/#overview","title":"Overview","text":"name description chain() Core funcchain syntax component to write chains Input Args prompt placeholders and input for your chains BaseModel Core component to create pydantic models/classes Settings Global settings object with for all your chains SettingsOverride Local settings dict for a specific chain OutputParser Parses the llm output into your desired shape Prompting Templating system and techniques for writing prompts Vision LLM that can also takes images as input/context Streaming Token by token streaming of llm output Unions Pydantic union types for your models LangChain Library for building cognitive systems"},{"location":"concepts/parser/","title":"Output Parser","text":""},{"location":"concepts/parser/#output-type-hints","title":"Output Type Hints","text":"<p>Funcchain recognises the output type hint you put on your function to automatically attach a fitting output parser to the end of your chain. This makes it really to code because you just use normal python typing syntax and funcchain handles everything for your.</p>"},{"location":"concepts/parser/#strings","title":"Strings","text":"<p>The simplest output type is a string. The output parser will return the content of the AI response just as it is.</p>"},{"location":"concepts/parser/#pydantic-models","title":"Pydantic Models","text":"<p>To force the model to respond in a certain way you can use pydantic models. This gives your alot of flexibility and control over the output because you can define the exact types of your fields and even add custom validation logic. Everything of your defined model will be part of the prompt including model_name, class_docstring, field_names, field_types and field_descriptions. This gives you alot of room for prompt engineering and optimisation.</p> <pre><code>from funcchain import chain\nfrom pydantic import BaseModel, Field\n\nclass GroceryList(BaseModel):\n    recipie: str = Field(description=\"Goal of what to cook with all items.\")\n    items: list[str] = Field(description=\"Items to buy\")\n\ndef create_grocerylist(customer_request: str) -&gt; GroceryList\n    \"\"\"\n    Come up with a grocery list based on what the customer wants.\n    \"\"\"\n    return chain()\n</code></pre> <p>When calling this function with e.g. <code>create_grocerylist(\"I want a cheap, protein rich and vegan meal.\")</code> the model is then forced to respond using the model as a json_schema and the unterlying conversation would look like the following:</p> <pre><code>&lt;HumanMessage&gt;\n    CUSTOMER_REQUEST:\n    I want a cheap, protein rich and vegan meal.\n\n    Come up with a grocery list based on what the customer wants.\n&lt;/HumanMessage&gt;\n\n&lt;AIMessage&gt;\n    {\n        \"recipie\": \"lentil soup\"\n        \"items\" [\n            \"todo\",\n            \"insert\",\n            \"ingredients\"\n        ]\n    }\n&lt;/AIMessage&gt;\n</code></pre> <p>This json is then automatically validated and parsed into the pydantic model. When a validation fails the model automatically recieves the error as followup message and tries again.</p>"},{"location":"concepts/parser/#primitive-types","title":"Primitive Types","text":"<p>You can also use other primitive types like int, float, bool, list, Literals, Enums, etc.  Funcchain will then create a temporary pydantic model with the type as a single field and use that as the output parser.</p> <pre><code>def create_grocerylist(customer_request: str) -&gt; list[str]\n    \"\"\"\n    Come up with a grocery list based on what the customer wants.\n    \"\"\"\n    return chain()\n</code></pre> <p>This time when calling this function with e.g. <code>create_grocerylist(\"I want a cheap, protein rich and vegan meal.\")</code> funcchain automatically creates a temporary pydantic model in the background like this:</p> <pre><code>class Extract(BaseModel):\n    value: list[str]\n</code></pre> <p>The model then understands the desired shape and will output like here:</p> <pre><code>&lt;AIMessage&gt;\n    {\n        \"value\": [\n            \"todo\",\n            \"insert\",\n            \"ingredients\"\n        ]\n    }\n&lt;/AIMessage&gt;\n</code></pre>"},{"location":"concepts/parser/#union-types","title":"Union Types","text":"<p>You can also use mupliple PydanticModels at once as the output type using Unions. The LLM will then select one of the models that fits best your inputs. Checkout the seperate page for UnionTypes for more info.</p>"},{"location":"concepts/parser/#streaming","title":"Streaming","text":"<p>You can stream everything with a <code>str</code> output type.</p> <p>Since pydantic models need to be fully constructed before they can be returned, you can't use them for streaming. There is one approach to stream pydantic models but it works only if all fields are Optional, which is not the case for most models and they still come field by field.</p> <p>This is not implemented yet but will be added in the future.</p>"},{"location":"concepts/prompting/","title":"Prompting","text":"<p>Prompting involes every text you write that the LLM recieves and interprets. It often involves <code>prompt engineering</code> which is optimizing and finetuning your wordings so the LLM understands what you wants and responds correctly. Everything from the input argument names, output type and docstring are part of the prompt and are visible to the model to evaluate. Make sure your choose your terms well and try different wordings if you encounter problems.</p>"},{"location":"concepts/prompting/#jinja2-templating","title":"Jinja2 Templating","text":"<p>Often you can write your funcchains without templating but for special cases it is useful to do custom things. Funcchain allows jinja2 as templating syntax for writing more complex prompts. All function input areguments that are either <code>str</code> or a subclass of a <code>Pydantic Model</code> are awailable in the jinja environment and can be used.</p> <pre><code>class GroceryList(BaseModel):\n    recipie: str\n    items: list[str]\n\ndef create_recipie(glist: GroceryList) -&gt; str:\n    \"\"\"\n    I want to cook {{ glist.recipie }}.\n    Create a step by step recipie based on these ingridients I just bought:\n    {% for i in glist.items %}\n    - {{ i }}\n    {% endfor %}\n    \"\"\"\n    return chain()\n</code></pre> <p>The LLM will then recieve a formatted prompt based on what you input is.</p>"},{"location":"concepts/prompting/#input-argument-placement","title":"Input Argument Placement","text":"<p>If you do not specify a place in your prompt for your input arguments using jinja, all unused arguments (<code>str</code> and <code>PydanticModels</code>) will then get automatically appended to the beginning of your instruction.</p> <p>E.g. if you just provide <code>Create a step by step recipie based on the grocery list.</code>, the prompt template would look like this:</p> <pre><code>&lt;PromptTemplate&gt;\n    GLIST:\n    {{\u00a0glist }}\n\n    Create a step by step recipie based on the grocery list.\n&lt;/PromptTemplate&gt;\n</code></pre> <p>When inserting the instance of <code>GroceryList</code> into the template, the <code>__str__</code> method is called for converting the model into text. Keep this in mind if you want to customise apperence to the LLM.</p>"},{"location":"concepts/prompting/#chatmodel-behavior","title":"ChatModel Behavior","text":"<p>Keep in mind that funcchain in always using instructional chat prompting, so instrution is made the perspective of a Human &lt;-&gt; AI conversation. If you process input from your users its good to talk of them as <code>customers</code> so the model understands the perspective.</p>"},{"location":"concepts/pydantic/","title":"Pydantic","text":"<p><code>pydantic</code> is a python library for creating data structures with strict typing and automatic type validation. When dealing with LLMs this is very useful because it exposes a precise <code>json-schema</code> that can be used with grammars or function-calling to force the LLM to respond in the desired way. Additionally validation especially using custom validators can be used to automatically retry if the output does not match your requirements.</p>"},{"location":"concepts/pydantic/#basemodel","title":"BaseModel","text":"<p>When <code>from pydantic import BaseModel</code> this is imports the core class of Pydantic which can be used to construct your data structures.</p> <pre><code>from pydantic import BaseModel\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n    items: list[str]\n</code></pre> <p>This model can then be initiated:</p> <pre><code>user = User(\n    id=1943,\n    name=\"Albert Hofmann\",\n    email=\"hofmann.albert@sandoz.ch\",\n    items=[\"lab coat\", \"safety glasses\", \"a bicycle\"]\n)\n</code></pre>"},{"location":"concepts/pydantic/#field-descriptions","title":"Field Descriptions","text":"<p>To give the LLM more context about what you mean with the stucture you can provide field descriptions:</p> <pre><code>from pydantic import Field\n\nclass User(BaseModel):\n    id: int\n    name: str = Field(description=\"FullName of the user.\")\n    email: str\n    items: list[str] = Field(description=\"Everyday items of the user.\")\n</code></pre> <p>These descriptions are included in the json-schema and are passed as format instructions into the prompt from the output parser.</p>"},{"location":"concepts/pydantic/#custom-validators","title":"Custom Validators","text":"<p>You can also write custom validators if you want to check for specific information beyond just the type.</p> <pre><code>from pydantic import field_validator\n\nclass User(BaseModel):\n    id: int\n    name: str = Field(description=\"FullName of the user.\")\n    email: str\n    items: list[str] = Field(description=\"Everyday items of the user.\")\n\n    @field_validator(\"email\")\n    def keywords_must_be_unique(cls, v: str) -&gt; str:\n        if not v.endswith(\"@sandoz.ch\"):\n            raise ValueError(\"User has to work at Sandoz to register!\")\n        return v\n</code></pre> <p>In this example the validator makes sure every user has an email ending with <code>@sandoz.ch</code>.</p>"},{"location":"concepts/streaming/","title":"Streaming","text":"<p>Streaming is important if you want to do things with your LLM generation while the LLM is still generating. This can enhance the user experience by already showing part of the response but you could also stop a generation early if it does not match certain requirements.</p>"},{"location":"concepts/streaming/#console-log-streaming","title":"Console Log Streaming","text":"<p>If you want to stream all the tokens generated quickly to your console output, you can use the <code>settings.console_stream = True</code> setting.</p>"},{"location":"concepts/streaming/#stream_to-wrapper","title":"<code>stream_to()</code> wrapper","text":"<p>For streaming with non runnable funcchains you can wrap the LLM generation call into the <code>stream_to()</code> context manager. This would look like this:</p> <pre><code>def summarize(text: str) -&gt; str:\n    \"\"\"Summarize the text.\"\"\"\n    return chain()\n\ntext = \"... a large text\"\n\nwith stream_to(print):\n    summarize(text)\n</code></pre> <p>This will call token by token the print function so it will show up in your console. But you can also insert any function that accepts a string to create your custom stream handlers.</p> <p>You can also use <code>async with stream_to(your_async_handler):</code> for async streaming. Make sure summarize is then created using <code>await achain()</code>.</p>"},{"location":"concepts/streaming/#langchain-runnable-streaming","title":"LangChain runnable streaming","text":"<p>If you can compile every funcchain into a langchain runnable and then use the native langchain syntax for streaming:</p> <pre><code>@runnable\ndef summarize(text: str) -&gt; str:\n    \"\"\"Summarize the text.\"\"\"\n    return chain()\n\ntext = \"... a large text\"\n\nfor chunk in summarize.stream(input={\"text\": text}):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"concepts/unions/","title":"Union Types","text":"<p>You can use union types in funcchain to make the model select one of multiple PydanticModels for the response. You may have seen this in the Complex Example.</p>"},{"location":"concepts/unions/#errors","title":"Errors","text":"<p>One good usecase for this is to always give the LLM the chance to raise an Error if the input is strange or not suited. You can check this in more detail here.</p>"},{"location":"concepts/unions/#agents","title":"Agents","text":"<p>Another usecase is to create an Agent like chain that selects one of multiple tools. Every PydanticModel then represents the input schema of your function and you can even override the <code>__call__</code> method of your models to directly execute the tool if you need so.</p>"},{"location":"concepts/unions/#function-calling","title":"Function Calling","text":"<p>Under the hood the union type featur uses openai tool_calling, especially the functionallity to give the LLM multiple tools to choose from. All pydantic models then get injected as available tools and the LLM is forced to call one of them.</p>"},{"location":"concepts/vision/","title":"Vision Models","text":"<p>Funcchain supports working with vision models so you can use images as input arguments of your prompts. This only works if you also choose the correct model. Currently known supported models:</p> <ul> <li><code>openai/gpt-4-vision-preview</code></li> <li><code>ollama/llava</code> or <code>ollama/bakllava</code></li> </ul> <p>You need to set these using <code>settings.llm</code> (checkout the Funcchain Settings).</p>"},{"location":"concepts/vision/#image-type","title":"Image Type","text":"<p><code>from funcchain import Image</code></p> <p>Funcchain introuces a special type for Images to quickly recognise image arguments and format them correctly into the prompt. This type also exposes a variaty of classmethods for creating but also methods for converting Image instances.</p> <p>Checkout the Vision Example for more details.</p>"},{"location":"contributing/codebase-structure/","title":"Codebase Structure","text":""},{"location":"contributing/codebase-structure/#todo-explain-structure-of-codebase-to-easier-contribute","title":"TODO: explain structure of codebase to easier contribute","text":""},{"location":"contributing/contributors/","title":"Contributors","text":"<p>We would like to acknowledge the contributions of the following people:</p> Name Contribution"},{"location":"contributing/contributors/#how-to-contribute","title":"How to Contribute","text":"<p>If you would like to contribute to this project, please follow the guidelines in our Contributing Guide.</p>"},{"location":"contributing/dev-setup/","title":"Development Setup","text":"<p>To contribute, clone the repo and run:</p> <pre><code>./dev_setup.sh\n</code></pre> <p>You should not run unstrusted scripts so ask ChatGPT to explain what the contents of this script do!</p> <p>This will install and setup your development environment using rye or pip.</p>"},{"location":"contributing/license/","title":"License","text":""},{"location":"contributing/license/#mit-license","title":"MIT License","text":"<p>All contributions are made under the MIT License. See LICENSE.md for more information.</p>"},{"location":"contributing/roadmap/","title":"TODOs for writing the documentation","text":"<ul> <li> <p> write out all todos in advanced</p> </li> <li> <p> look more into other repos for mkdocs tricks and inspiration</p> </li> <li> <p> make this file a general todo list + roadmap for contributors</p> </li> <li> <p> maybe rename features to examples</p> </li> </ul>"},{"location":"contributing/security/","title":"Security","text":"<p>If you notice any security risks please immidiatly email <code>contact@shroominic.com</code> and for major risks you will recieve a bounty of 100$.</p>"},{"location":"features/dynamic_router/","title":"Dynamic Router","text":""},{"location":"features/dynamic_router/#dynamic-chat-router-with-funcchain","title":"Dynamic Chat Router with Funcchain","text":"<p>Example</p> <p>dynamic_router.py Example</p> <p>In this example we will use funcchain to build a LLM routing pipeline. This is a very useful LLM task and can be used in a variety of applications. You can abstract this for your own usage. This should serve as an example of how to archive complex structures using funcchain.</p> <p>A dynamic chat router that selects the appropriate handler for user queries based on predefined routes.</p>"},{"location":"features/dynamic_router/#full-code-example","title":"Full Code Example","text":"<pre><code>from enum import Enum\nfrom typing import Any, Callable, TypedDict\n\nfrom funcchain.syntax.executable import compile_runnable\nfrom pydantic import BaseModel, Field\n\n\nclass Route(TypedDict):\n    handler: Callable\n    description: str\n\n\nclass DynamicChatRouter(BaseModel):\n    routes: dict[str, Route]\n\n    def _routes_repr(self) -&gt; str:\n        return \"\\n\".join([f\"{route_name}: {route['description']}\" for route_name, route in self.routes.items()])\n\n    def invoke_route(self, user_query: str, /, **kwargs: Any) -&gt; Any:\n        RouteChoices = Enum(  # type: ignore\n            \"RouteChoices\",\n            {r: r for r in self.routes.keys()},\n            type=str,\n        )\n\n        class RouterModel(BaseModel):\n            selector: RouteChoices = Field(\n                default=\"default\",\n                description=\"Enum of the available routes.\",\n            )\n\n        route_query = compile_runnable(\n            instruction=\"Given the user query select the best query handler for it.\",\n            input_args=[\"user_query\", \"query_handlers\"],\n            output_type=RouterModel,\n        )\n\n        selected_route = route_query.invoke(\n            input={\n                \"user_query\": user_query,\n                \"query_handlers\": self._routes_repr(),\n            }\n        ).selector\n        assert isinstance(selected_route, str)\n\n        return self.routes[selected_route][\"handler\"](user_query, **kwargs)\n\n\ndef handle_pdf_requests(user_query: str) -&gt; str:\n    return \"Handling PDF requests with user query: \" + user_query\n\n\ndef handle_csv_requests(user_query: str) -&gt; str:\n    return \"Handling CSV requests with user query: \" + user_query\n\n\ndef handle_default_requests(user_query: str) -&gt; str:\n    return \"Handling DEFAULT requests with user query: \" + user_query\n\n\nrouter = DynamicChatRouter(\n    routes={\n        \"pdf\": {\n            \"handler\": handle_pdf_requests,\n            \"description\": \"Call this for requests including PDF Files.\",\n        },\n        \"csv\": {\n            \"handler\": handle_csv_requests,\n            \"description\": \"Call this for requests including CSV Files.\",\n        },\n        \"default\": {\n            \"handler\": handle_default_requests,\n            \"description\": \"Call this for all other requests.\",\n        },\n    },\n)\n\n\nrouter.invoke_route(\"Can you summarize this csv?\")\n</code></pre>"},{"location":"features/dynamic_router/#demo","title":"Demo","text":"<pre><code>$ router.invoke_route(\"Can you summarize this csv?\")\nHandling CSV requests with user query: Can you summarize this csv?\n</code></pre>"},{"location":"features/dynamic_router/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>Nececary imports</p> <pre><code>from enum import Enum\nfrom typing import Any, Callable, TypedDict\n\nfrom funcchain.syntax.executable import compile_runnable\nfrom pydantic import BaseModel, Field\n</code></pre> <p>Define Route Type</p> <pre><code>class Route(TypedDict):\n    handler: Callable\n    description: str\n</code></pre> <p>Create a <code>TypedDict</code> to define the structure of a route with a handler function and a description. Just leave this unchanged if not intentionally experimenting.</p> <p>Implement Route Representation</p> <p>Establish a Router class</p> <pre><code>class DynamicChatRouter(BaseModel):\n    routes: dict[str, Route]\n</code></pre> <p>_routes_repr():</p> <p>Returns a string representation of all routes and their descriptions, used to help the language model understand the available routes.</p> <pre><code>def _routes_repr(self) -&gt; str:\n    return \"\\n\".join([f\"{route_name}: {route['description']}\" for route_name, route in self.routes.items()])\n</code></pre> <p>**invoke_route(user_query: str, **kwargs: Any) -&gt; Any: **</p> <p>This method takes a user query and additional keyword arguments. Inside invoke_route, an Enum named RouteChoices is dynamically created with keys corresponding to the route names. This Enum is used to validate the selected route.</p> <pre><code>def invoke_route(self, user_query: str, /, **kwargs: Any) -&gt; Any:\n    RouteChoices = Enum(  # type: ignore\n        \"RouteChoices\",\n        {r: r for r in self.routes.keys()},\n        type=str,\n    )\n</code></pre> <p>Compile the Route Selection Logic</p> <p>The <code>RouterModel</code> class in this example is used for defining the expected output structure that the <code>compile_runnable</code> function will use to determine the best route for a given user query.</p> <pre><code>class RouterModel(BaseModel):\n    selector: RouteChoices = Field(\n        default=\"default\",\n        description=\"Enum of the available routes.\",\n    )\n\nroute_query = compile_runnable(\n    instruction=\"Given the user query select the best query handler for it.\",\n    input_args=[\"user_query\", \"query_handlers\"],\n    output_type=RouterModel,\n)\n\nselected_route = route_query.invoke(\n    input={\n        \"user_query\": user_query,\n        \"query_handlers\": self._routes_repr(),\n    }\n).selector\nassert isinstance(selected_route, str)\n\nreturn self.routes[selected_route][\"handler\"](user_query, **kwargs)\n</code></pre> <ul> <li><code>RouterModel</code>: Holds the route selection with a default option, ready for you to play around with.</li> <li><code>RouteChoices</code>: An Enum built from route names, ensuring you only get valid route selections.</li> <li><code>compile_runnable</code>: Sets up the decision-making logic for route selection, guided by the provided instruction and inputs.</li> <li><code>route_query</code>: Calls the decision logic with the user's query and a string of route descriptions.</li> <li><code>selected_route</code>: The outcome of the decision logic, representing the route to take.</li> <li><code>assert</code>: A safety check to confirm the route is a string, as expected by the routes dictionary.</li> <li><code>handler invocation</code>: Runs the chosen route's handler with the provided query and additional arguments.</li> </ul> <p>Define route functions</p> <p>Now you can use the structured output to execute programatically based on a natural language input. Establish functions tailored to your needs.</p> <pre><code>def handle_pdf_requests(user_query: str) -&gt; str:\nreturn \"Handling PDF requests with user query: \" + user_query\n\ndef handle_csv_requests(user_query: str) -&gt; str:\n    return \"Handling CSV requests with user query: \" + user_query\n\ndef handle_default_requests(user_query: str) -&gt; str:\n    return \"Handling DEFAULT requests with user query: \" + user_query\n</code></pre> <p>Define the routes</p> <p>And bind the previous established functions.</p> <pre><code>router = DynamicChatRouter(\n    routes={\n        \"pdf\": {\n            \"handler\": handle_pdf_requests,\n            \"description\": \"Call this for requests including PDF Files.\",\n        },\n        \"csv\": {\n            \"handler\": handle_csv_requests,\n            \"description\": \"Call this for requests including CSV Files.\",\n        },\n        \"default\": {\n            \"handler\": handle_default_requests,\n            \"description\": \"Call this for all other requests.\",\n        },\n    },\n)\n</code></pre> <p>Get output</p> <p>Use the router.invoke_route method to process the user query and obtain the appropriate response.</p> <pre><code>router.invoke_route(\"Can you summarize this csv?\")\n</code></pre>"},{"location":"features/enums/","title":"Enums","text":""},{"location":"features/enums/#decision-making-with-enums-and-funcchain","title":"Decision Making with Enums and Funcchain","text":"<p>Example</p> <p>See enums.py</p> <p>In this example, we will use the enum module and funcchain library to build a decision-making system. This is a useful task for creating applications that require predefined choices or responses. You can adapt this for your own usage. This serves as an example of how to implement decision-making logic using enums and the funcchain library.</p>"},{"location":"features/enums/#full-code-example","title":"Full Code Example","text":"<p>A simple system that takes a question and decides a 'yes' or 'no' answer based on the input.</p> <pre><code>\n<pre><code>from enum import Enum\nfrom funcchain import chain\nfrom pydantic import BaseModel\n\nclass Answer(str, Enum):\n    yes = \"yes\"\n    no = \"no\"\n\nclass Decision(BaseModel):\n    answer: Answer\n\ndef make_decision(question: str) -&gt; Decision:\n    \"\"\"\n    Based on the question decide yes or no.\n    \"\"\"\n    return chain()\n\nprint(make_decision(\"Do you like apples?\"))\n</code></pre>\n</code></pre>"},{"location":"features/enums/#demo","title":"Demo","text":"<pre><code>$ make_decision(\"Do you like apples?\")\n\nanswer=&lt;Answer.yes: 'yes'&gt;\n</code></pre>"},{"location":"features/enums/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>Necessary Imports</p> <pre><code>from enum import Enum\nfrom funcchain import chain\nfrom pydantic import BaseModel\n</code></pre> <p>Define the Answer Enum</p> <p>The Answer enum defines possible answers as 'yes' and 'no', which are the only valid responses for the decision-making system. Experiment by using and describing other enums.</p> <pre><code>class Answer(str, Enum):\n    yes = \"yes\"\n    no = \"no\"\n</code></pre> <p>Create the Decision Model</p> <p>The Decision class uses Pydantic to model a decision, ensuring that the answer is always an instance of the Answer enum.</p> <pre><code>class Decision(BaseModel):\n    answer: Answer\n</code></pre> <p>Implement the Decision Function</p> <p>The make_decision function is where the decision logic will be implemented, using <code>chain()</code> to process the question and return a decision. When using your own enums you want to edit this accordingly.</p> <pre><code>def make_decision(question: str) -&gt; Decision:\n    \"\"\"\n    Based on the question decide yes or no.\n    \"\"\"\n    return chain()\n</code></pre> <p>Run the Decision System</p> <p>This block runs the decision-making system, printing out the decision for a given question when the script is executed directly.</p> <pre><code>print(make_decision(\"Do you like apples?\"))\n</code></pre>"},{"location":"features/error_output/","title":"Error Output","text":""},{"location":"features/error_output/#example-of-raising-an-error","title":"Example of raising an error","text":"<p>Example</p> <p>error_output.py Example</p> <p>In this example, we will use the funcchain library to build a system that extracts user information from text. Most importantly we will be able to raise an error thats programmatically usable. You can adapt this for your own usage.</p> <p>The main functionality is to take a string of text and attempt to extract user information, such as name and email, and return a User object. If the information is insufficient, an Error is returned instead.</p>"},{"location":"features/error_output/#full-code-example","title":"Full Code Example","text":"<pre><code>\n<pre><code>from funcchain import BaseModel, Error, chain\nfrom rich import print\n\nclass User(BaseModel):\n    name: str\n    email: str | None\n\ndef extract_user_info(text: str) -&gt; User | Error:\n    \"\"\"\n    Extract the user information from the given text.\n    In case you do not have enough infos, raise.\n    \"\"\"\n    return chain()\n\nprint(extract_user_info(\"hey\"))\n# =&gt; returns Error\n\nprint(extract_user_info(\"I'm John and my mail is john@gmail.com\"))\n# =&gt; returns a User object\n</code></pre>\n</code></pre> <p>Demo</p> <pre><code>$ extract_user_info(\"hey\")\n\nError(\n    title='Invalid Input',\n    description='The input text does not contain user information.'\n)\n\n$ extract_user_info(\"I'm John and my mail is john@gmail.com\")\n\nUser(\n    name='John',\n    email='john@gmail.com'\n)\n</code></pre>"},{"location":"features/error_output/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>Necessary Imports</p> <pre><code>from funcchain import BaseModel, Error, chain\nfrom rich import print\n</code></pre> <p>Define the User Model</p> <p><pre><code>class User(BaseModel):\n    name: str\n    email: str | None\n</code></pre> The User class is a Pydantic model that defines the structure of the user information to be extracted, with fields for <code>name</code> and an email. Change the fields to experiment and alignment with your project.</p> <p>Implement the Extraction Function</p> <p>The <code>extract_user_info</code> function is intended to process the input text and return either a User object with extracted information or an Error if the information is not sufficient.</p> <p><pre><code>def extract_user_info(text: str) -&gt; User | Error:\n    \"\"\"\n    Extract the user information from the given text.\n    In case you do not have enough infos, raise.\n    \"\"\"\n    return chain()\n</code></pre> For experiments and adoptions also change the <code>str</code> that will be used in chain() to identify what you defined earlier in the <code>User(BaseModel)</code></p> <p>Run the Extraction System</p> <p>This conditional block is used to execute the extraction function and print the results when the script is run directly.</p> <pre><code>print(extract_user_info(\"hey\"))\n# =&gt; returns Error\n\nprint(extract_user_info(\"I'm John and my mail is john@gmail.com\"))\n# =&gt; returns a User object\n</code></pre>"},{"location":"features/literals/","title":"Literals","text":""},{"location":"features/literals/#literal-type-enforcement-in-funcchain","title":"Literal Type Enforcement in Funcchain","text":"<p>Example</p> <p>literals.py Example</p> <p>This is a useful task for scenarios where you want to ensure that certain outputs strictly conform to a predefined set of values. This serves as an example of how to implement strict type checks on outputs using the Literal type from the typing module and the funcchain library.</p> <p>You can adapt this for your own usage.</p>"},{"location":"features/literals/#full-code-example","title":"Full Code Example","text":"<pre><code>\n<pre><code>from typing import Literal\nfrom funcchain import chain\nfrom pydantic import BaseModel\n\nclass Ranking(BaseModel):\n    analysis: str\n    score: Literal[11, 22, 33, 44, 55]\n    error: Literal[\"no_input\", \"all_good\", \"invalid\"]\n\ndef rank_output(output: str) -&gt; Ranking:\n    \"\"\"\n    Analyze and rank the output.\n    \"\"\"\n    return chain()\n\nrank = rank_output(\"The quick brown fox jumps over the lazy dog.\")\nprint(rank)\n</code></pre>\n</code></pre> <p>Demo</p> <pre><code>rank = rank_output(\"The quick brown fox jumps over the lazy dog.\")\nprint(rank)\n$ ........\nRanking(analysis='...', score=33, error='all_good')\n</code></pre>"},{"location":"features/literals/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>Necessary Imports</p> <pre><code>from typing import Literal\nfrom funcchain import chain\nfrom pydantic import BaseModel\n</code></pre> <p>Define the Ranking Model</p> <p>The Ranking class is a Pydantic model that uses the Literal type to ensure that the score and error fields can only contain certain predefined values. So experiment with changing those but keeping this structure of the class. The LLM will be forced to deliver one of the defined output.</p> <pre><code>class Ranking(BaseModel):\n    analysis: str\n    score: Literal[11, 22, 33, 44, 55]\n    error: Literal[\"no_input\", \"all_good\", \"invalid\"]\n</code></pre> <p>Implement the Ranking Function</p> <p>Use <code>chain()</code> to process a user input, which must be a string. Adjust the content based on your above defined class.</p> <pre><code>def rank_output(output: str) -&gt; Ranking:\n \"\"\"\n Analyze and rank the output.\n \"\"\"\n return chain()\n</code></pre> <p>Execute the Ranking System</p> <p>This block is used to execute the ranking function and print the results when the script is run directly.</p> <pre><code>rank = rank_output(\"The quick brown fox jumps over the lazy dog.\")\nprint(rank)\n</code></pre>"},{"location":"features/llamacpp/","title":"LlamaCpp","text":""},{"location":"features/llamacpp/#different-llms-with-funcchain-easy-to-use","title":"Different LLMs with funcchain EASY TO USE","text":"<p>Example</p> <p>See llamacpp.py Also see supported MODELS.md</p> <p>In this example, we will use the funcchain library to perform sentiment analysis on a piece of text. This showcases how funcchain can seamlessly utilize different Language Models (LLMs) using local llamacpp models, without many code changes..</p> <p>This is particularly useful for developers looking to integrate different models in a single application or just experimenting with different models.</p>"},{"location":"features/llamacpp/#full-code-example","title":"Full Code Example","text":"<pre><code>\n<pre><code>from funcchain import chain, settings\nfrom pydantic import BaseModel, Field\nfrom rich import print\n\n# define your model\nclass SentimentAnalysis(BaseModel):\n    analysis: str = Field(description=\"A description of the analysis\")\n    sentiment: bool = Field(description=\"True for Happy, False for Sad\")\n\n# define your prompt\ndef analyze(text: str) -&gt; SentimentAnalysis:\n    \"\"\"\n    Determines the sentiment of the text.\n    \"\"\"\n    return chain()\n\n# set global llm\nsettings.llm = \"llamacpp/openchat-3.5-0106:Q3_K_M\"\n\n# log tokens as stream to console\nsettings.console_stream = True\n\n# run prompt\npoem = analyze(\"I really like when my dog does a trick!\")\n\n# show final parsed output\nprint(poem)\n</code></pre>\n</code></pre>"},{"location":"features/llamacpp/#demo","title":"Demo","text":"<pre><code>poem = analyze(\"I really like when my dog does a trick!\")\n\n$ {\"analysis\": \"A dog trick\", \"sentiment\": true}\n\nSentimentAnalysis(analysis='A dog trick', sentiment=True)\n</code></pre>"},{"location":"features/llamacpp/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>Necessary Imports</p> <pre><code>from funcchain import chain, settings\nfrom pydantic import BaseModel, Field\n</code></pre> <p>Define the Data Model</p> <p>Here, we define a <code>SentimentAnalysis</code> model with a description of the sentiment analysis and a boolean field indicating the sentiment.</p> <pre><code>class SentimentAnalysis(BaseModel):\n    analysis: str = Field(description=\"A description of the analysis\")\n    sentiment: bool = Field(description=\"True for Happy, False for Sad\")\n</code></pre> <p>Create the Analysis Function</p> <p>This 'analyze' function takes a string as input and is expected to return a <code>SentimentAnalysis</code> object by calling the <code>chain()</code> function from the <code>funcchain</code> library.</p> <pre><code>def analyze(text: str) -&gt; SentimentAnalysis:\n    \"\"\"\n    Determines the sentiment of the text.\n    \"\"\"\n    return chain()\n</code></pre> <p>Execution Configuration</p> <p>In the main block, configure the global settings to set the preferred LLM, enable console streaming, and run the <code>analyze</code> function with sample text.</p> <pre><code># set global llm\nsettings.llm = \"llamacpp/openchat-3.5-0106:Q3_K_M\"\n\n# log tokens as stream to console\nsettings.console_stream = True\n\n# run prompt\npoem = analyze(\"I really like when my dog does a trick!\")\n\n# show final parsed output\nprint(poem)\n</code></pre> <p>Important</p> <p>We need to note here is that <code>settings.llm</code> can be adjusted to any model mentioned in MODELS.md and your funcchain code will still work and <code>chain()</code> does everything in the background for you.</p>"},{"location":"features/ollama/","title":"Ollama","text":""},{"location":"features/ollama/#different-llms-with-funcchain-easy-to-use","title":"Different LLMs with funcchain EASY TO USE","text":"<p>Example</p> <p>See ollama.py Also see supported MODELS.md</p> <p>In this example, we will use the funcchain library to perform sentiment analysis on a piece of text. This showcases how funcchain can seamlessly utilize different Language Models (LLMs) from ollama, without many unnececary code changes..</p> <p>This is particularly useful for developers looking to integrate different models in a single application or just experimenting with different models.</p>"},{"location":"features/ollama/#full-code-example","title":"Full Code Example","text":"<pre><code>\n<pre><code>from funcchain import chain, settings\nfrom pydantic import BaseModel, Field\n\n# define your model\nclass SentimentAnalysis(BaseModel):\n    analysis: str = Field(description=\"A description of the analysis\")\n    sentiment: bool = Field(description=\"True for Happy, False for Sad\")\n\n# define your prompt\ndef analyze(text: str) -&gt; SentimentAnalysis:\n    \"\"\"\n    Determines the sentiment of the text.\n    \"\"\"\n    return chain()\n\nif __name__ == \"__main__\":\n    # set global llm\n    settings.llm = \"ollama/openchat\"\n\n    # log tokens as stream to console\n    settings.console_stream = True\n\n    # run prompt\n    poem = analyze(\"I really like when my dog does a trick!\")\n\n    # show final parsed output\n    print(poem)\n</code></pre>\n</code></pre>"},{"location":"features/ollama/#demo","title":"Demo","text":"<pre><code>poem = analyze(\"I really like when my dog does a trick!\")\n\n$ {\"analysis\": \"A dog trick\", \"sentiment\": true}\n\nSentimentAnalysis(analysis='A dog trick', sentiment=True)\n</code></pre>"},{"location":"features/ollama/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>Necessary Imports</p> <pre><code>from funcchain import chain, settings\nfrom pydantic import BaseModel, Field\n</code></pre> <p>Define the Data Model Here, we define a <code>SentimentAnalysis</code> model with a description of the sentiment analysis and a boolean field indicating the sentiment.</p> <pre><code>class SentimentAnalysis(BaseModel):\n    analysis: str = Field(description=\"A description of the analysis\")\n    sentiment: bool = Field(description=\"True for Happy, False for Sad\")\n</code></pre> <p>Create the Analysis Function</p> <p>This 'analyze' function takes a string as input and is expected to return a <code>SentimentAnalysis</code> object by calling the <code>chain()</code> function from the <code>funcchain</code> library.</p> <pre><code>def analyze(text: str) -&gt; SentimentAnalysis:\n    \"\"\"\n    Determines the sentiment of the text.\n    \"\"\"\n    return chain()\n</code></pre> <p>Execution Configuration</p> <p>In the main block, configure the global settings to set the preferred LLM, enable console streaming, and run the <code>analyze</code> function with sample text.</p> <pre><code>settings.llm = \"ollama/openchat\"\nsettings.console_stream = True\npoem = analyze(\"I really like when my dog does a trick!\")\nprint(poem)\n</code></pre> <p>Important</p> <p>We need to note here is that <code>settings.llm</code> can be adjusted to any model mentioned in MODELS.md and your funcchain code will still work and <code>chain()</code> does everything in the background for you.</p>"},{"location":"features/openai_json_mode/","title":"OpenAI JSON Output","text":""},{"location":"features/openai_json_mode/#json-structured-output-using-funcchain-with-oenai","title":"JSON structured Output using Funcchain with OenAI","text":"<p>Example</p> <p>See openai_json_mode.py</p> <p>This example will showcase how funcchain enables OpenAI to output even the type <code>int</code> as JSON.</p> <p>This example demonstrates using the funcchain library and pydantic to create a FruitSalad model, sum its contents, and output the total in a Result model as an integer.</p>"},{"location":"features/openai_json_mode/#full-code-example","title":"Full Code Example","text":"<pre><code>\n<pre><code>from funcchain import chain\nfrom pydantic import BaseModel\n\nclass FruitSalad(BaseModel):\n    bananas: int = 0\n    apples: int = 0\n\ndef sum_fruits(fruit_salad: FruitSalad) -&gt; int:\n    \"\"\"\n    Sum the number of fruits in a fruit salad.\n    \"\"\"\n    return chain()\n\nif __name__ == \"__main__\":\n    fruit_salad = FruitSalad(bananas=3, apples=5)\n    assert sum_fruits(fruit_salad) == 8\n</code></pre>\n</code></pre> <p>Instructions</p> <p>Step-by-step</p> <p>Necessary Imports</p> <p><code>funcchain</code> for chaining functionality, and <code>pydantic</code> for the data models.</p> <pre><code>from funcchain import chain, settings\nfrom pydantic import BaseModel\n</code></pre> <p>Defining the Data Models</p> <p>We define two Pydantic models: <code>FruitSalad</code> with integer fields for the number of bananas and apples. Of course feel free to change those classes according to your needs but use of <code>pydantic</code> is required.</p> <pre><code>class FruitSalad(BaseModel):\n    bananas: int = 0\n    apples: int = 0\n</code></pre> <p>Summing Function</p> <p>The <code>sum_fruits</code> function is intended to take a <code>FruitSalad</code> object and use <code>chain()</code> for solving this task with an LLM. The result is returned then returned as integer.</p> <pre><code>def sum_fruits(fruit_salad: FruitSalad) -&gt; int:\n    \"\"\"\n    Sum the number of fruits in a fruit salad.\n    \"\"\"\n    return chain()\n</code></pre> <p>Execution Block</p> <pre><code>fruit_salad = FruitSalad(bananas=3, apples=5)\nassert sum_fruits(fruit_salad) == 8\n</code></pre> <p>In the primary execution section of the script, we instantiate a <code>FruitSalad</code> object with predefined quantities of bananas and apples. We then verify that the <code>sum_fruits</code> function accurately calculates the total count of fruits, which should be 8 in this case.</p>"},{"location":"features/retry_parsing/","title":"Retry Parsing","text":""},{"location":"features/retry_parsing/#retry-parsing","title":"Retry Parsing","text":"<p>Example</p> <p>pydantic_validation.py</p> <p>You can adapt this for your own usage. This serves as an example of how to implement data validation and task creation using pydantic for data models and funcchain for processing natural language input.</p> <p>The main functionality is to parse a user description, validate the task details, and create a new Task object with unique keywords and a difficulty level within a specified range.</p>"},{"location":"features/retry_parsing/#full-code-example","title":"Full Code Example","text":"<pre><code>\n<pre><code>from funcchain import chain, settings\nfrom pydantic import BaseModel, field_validator\n\n# settings.llm = \"ollama/openchat\"\nsettings.console_stream = True\n\nclass Task(BaseModel):\n    name: str\n    difficulty: int\n    keywords: list[str]\n\n    @field_validator(\"keywords\")\n    def keywords_must_be_unique(cls, v: list[str]) -&gt; list[str]:\n        if len(v) != len(set(v)):\n            raise ValueError(\"keywords must be unique\")\n        return v\n\n    @field_validator(\"difficulty\")\n    def difficulty_must_be_between_1_and_10(cls, v: int) -&gt; int:\n        if v &lt; 10 or v &gt; 100:\n            raise ValueError(\"difficulty must be between 10 and 100\")\n        return v\n\ndef gather_infos(user_description: str) -&gt; Task:\n    \"\"\"\n    Based on the user description,\n    create a new task to put on the todo list.\n    \"\"\"\n    return chain()\n\nif __name__ == \"__main__\":\n    task = gather_infos(\"cleanup the kitchen\")\n    print(f\"{task=}\")\n</code></pre>\n</code></pre> <p>Demo</p> <pre><code>User:\n$ cleanup the kitchen\n\ntask=Task\nname='cleanup',\ndifficulty=30,\nkeywords=['kitchen', 'cleanup']\n</code></pre>"},{"location":"features/retry_parsing/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>Necessary Imports</p> <pre><code>from funcchain import chain, settings\nfrom pydantic import BaseModel, field_validator\n</code></pre> <p>Define the Task Model with Validators The <code>Task</code> class is a Pydantic model with fields: <code>name</code>, <code>difficulty</code>, and <code>keywords</code>. Validators ensure data integrity:</p> <ul> <li><code>keywords_must_be_unique</code>: Checks that all keywords are distinct.</li> <li><code>difficulty_must_be_between_1_and_10</code>: Ensures difficulty is within 10 to 100.</li> </ul> <pre><code>class Task(BaseModel):\n    name: str  # Task name.\n    difficulty: int  # Difficulty level (10-100).\n    keywords: list[str]  # Unique keywords.\n\n    @field_validator(\"keywords\")\n    def keywords_must_be_unique(cls, v: list[str]) -&gt; list[str]:\n        # Ensure keyword uniqueness.\n        if len(v) != len(set(v)):\n            raise ValueError(\"keywords must be unique\")\n        return v\n\n    @field_validator(\"difficulty\")\n    def difficulty_must_be_between_1_and_10(cls, v: int) -&gt; int:\n        # Validate difficulty range.\n        if v &lt; 10 or v &gt; 100:\n            raise ValueError(\"difficulty must be between 10 and 100\")\n        return v\n</code></pre> <p>Implement the Information Gathering Function The gather_infos function is designed to take a user description and use the chain function to process and validate the input, returning a new Task object. Adjust the string description to match your purposes when changing the code above.</p> <pre><code>def gather_infos(user_description: str) -&gt; Task:\n    \"\"\"\n    Based on the user description,\n    create a new task to put on the todo list.\n    \"\"\"\n    return chain()\n</code></pre> <p>Execute the Script Runs gather_infos with a sample and prints the Task. <pre><code>if __name__ == \"__main__\":\n    task = gather_infos(\"cleanup the kitchen\")\n    print(f\"{task=}\")\n</code></pre></p>"},{"location":"features/static_router/","title":"Static Router","text":""},{"location":"features/static_router/#static-routing-with-funcchain-and-pydantic","title":"Static Routing with Funcchain and Pydantic","text":"<p>Example</p> <p>See static_router.py</p> <p>This serves as an example of how to implement static routing using funcchain for decision-making and Enum for route selection. This is a useful task for applications that need to route user requests to specific handlers based on the content of the request. You can adapt this for your own usage.</p>"},{"location":"features/static_router/#full-code-example","title":"Full Code Example","text":"<pre><code>\n<pre><code>from enum import Enum\nfrom typing import Any\n\nfrom funcchain import chain, settings\nfrom pydantic import BaseModel, Field\n\nsettings.console_stream = True\n\ndef handle_pdf_requests(user_query: str) -&gt; None:\n    print(\"Handling PDF requests with user query: \", user_query)\n\ndef handle_csv_requests(user_query: str) -&gt; None:\n    print(\"Handling CSV requests with user query: \", user_query)\n\ndef handle_default_requests(user_query: str) -&gt; Any:\n    print(\"Handling DEFAULT requests with user query: \", user_query)\n\nclass RouteChoices(str, Enum):\n    pdf = \"pdf\"\n    csv = \"csv\"\n    default = \"default\"\n\nclass Router(BaseModel):\n    selector: RouteChoices = Field(description=\"Enum of the available routes.\")\n\n    def invoke_route(self, user_query: str) -&gt; Any:\n        match self.selector.value:\n            case RouteChoices.pdf:\n                return handle_pdf_requests(user_query)\n            case RouteChoices.csv:\n                return handle_csv_requests(user_query)\n            case RouteChoices.default:\n                return handle_default_requests(user_query)\n\ndef route_query(user_query: str) -&gt; Router:\n    return chain()\n\nuser_query = input(\"Enter your query: \")\nrouted_chain = route_query(user_query)\nrouted_chain.invoke_route(user_query)\n</code></pre>\n</code></pre> <p>Demo</p> <pre><code>Enter your query:\n$ I need to process a CSV file\n\nHandling CSV requests with user query: I need to process a CSV file\n</code></pre>"},{"location":"features/static_router/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>We will implement a script with the functionality to take a user query, determine the type of request (PDF, CSV, or default), and invoke the appropriate handler function.</p> <p>Necessary Imports</p> <pre><code>from enum import Enum\nfrom typing import Any\nfrom funcchain import chain, settings\nfrom pydantic import BaseModel, Field\n</code></pre> <p>Define Route Handlers</p> <p>These functions are the specific handlers for different types of user queries.</p> <pre><code>def handle_pdf_requests(user_query: str) -&gt; None:\n    print(\"Handling PDF requests with user query: \", user_query)\n\ndef handle_csv_requests(user_query: str) -&gt; None:\n    print(\"Handling CSV requests with user query: \", user_query)\n\ndef handle_default_requests(user_query: str) -&gt; Any:\n    print(\"Handling DEFAULT requests with user query: \", user_query)\n</code></pre> <p>Create RouteChoices Enum and Router Model</p> <p>RouteChoices is an Enum that defines the possible routes. Router is a Pydantic model that selects and invokes the appropriate handler based on the route.</p> <pre><code>class RouteChoices(str, Enum):\n    pdf = \"pdf\"\n    csv = \"csv\"\n    default = \"default\"\n\nclass Router(BaseModel):\n    selector: RouteChoices = Field(description=\"Enum of the available routes.\")\n\n    def invoke_route(self, user_query: str) -&gt; Any:\n        match self.selector.value:\n            case RouteChoices.pdf:\n                return handle_pdf_requests(user_query)\n            case RouteChoices.csv:\n                return handle_csv_requests(user_query)\n            case RouteChoices.default:\n                return handle_default_requests(user_query)\n</code></pre> <p>Implement Routing Logic</p> <p>The route_query function is intended to determine the best route for a given user query using the <code>chain()</code> function.</p> <pre><code>def route_query(user_query: str) -&gt; Router:\n    return chain()\n</code></pre> <p>Execute the Routing System</p> <p>This block runs the routing system, asking the user for a query and then processing it through the defined routing logic.</p> <pre><code>user_query = input(\"Enter your query: \")\nrouted_chain = route_query(user_query)\nrouted_chain.invoke_route(user_query)\n</code></pre>"},{"location":"features/stream/","title":"Streaming Output","text":""},{"location":"features/stream/#streaming-with-funcchain","title":"Streaming with Funcchain","text":"<p>Example</p> <p>See stream.py</p> <p>This serves as an example of how to implement streaming output for text generation tasks using funcchain.</p>"},{"location":"features/stream/#full-code-example","title":"Full Code Example","text":"<pre><code>\n<pre><code>from funcchain import chain, settings\nfrom funcchain.backend.streaming import stream_to\n\nsettings.temperature = 1\n\ndef generate_story_of(topic: str) -&gt; str:\n    \"\"\"\n    Write a short story based on the topic.\n    \"\"\"\n    return chain()\n\nwith stream_to(print):\n    generate_story_of(\"a space cat\")\n</code></pre>\n</code></pre> <p>Demo</p> <pre><code>with stream_to(print):\n    generate_story_of(\"a space cat\")\n\n$ Once upon a time in a galaxy far, far away, there was a space cat named Whiskertron...\n</code></pre>"},{"location":"features/stream/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>Necessary Imports</p> <pre><code>from funcchain import chain, settings\nfrom funcchain.backend.streaming import stream_to\n</code></pre> <p>Configure Settings</p> <p>The settings are configured to set the temperature, which controls the creativity of the language model's output. Experiment with different values.</p> <pre><code>settings.temperature = 1\n</code></pre> <p>Define the Story Generation Function</p> <p>The generate_story_of function is designed to take a topic and use the chain function to generate a story.</p> <pre><code>def generate_story_of(topic: str) -&gt; str:\n    \"\"\"\n    Write a short story based on the topic.\n    \"\"\"\n    return chain()\n</code></pre> <p>Execute the Streaming Generation</p> <p>This block uses the stream_to context manager to print the output of the story generation function as it is being streamed. This is how you stream the story while it is being generated.</p> <pre><code>with stream_to(print):\n    generate_story_of(\"a space cat\")\n</code></pre>"},{"location":"features/vision/","title":"Structured vision output","text":""},{"location":"features/vision/#image-analysis-with-funcchain-and-pydantic","title":"Image Analysis with Funcchain and Pydantic","text":"<p>Example</p> <p>vision.py</p> <p>This is a useful task for applications that need to extract structured information from images. You can adapt this for your own usage. This serves as an example of how to implement image analysis using the funcchain library's integration with openai/gpt-4-vision-preview.</p>"},{"location":"features/vision/#full-code-example","title":"Full Code Example","text":"<pre><code>\n<pre><code>from funcchain import Image, chain, settings\nfrom pydantic import BaseModel, Field\n\nsettings.llm = \"openai/gpt-4-vision-preview\"\nsettings.console_stream = True\n\nclass AnalysisResult(BaseModel):\n    \"\"\"The result of an image analysis.\"\"\"\n\n    theme: str = Field(description=\"The theme of the image\")\n    description: str = Field(description=\"A description of the image\")\n    objects: list[str] = Field(description=\"A list of objects found in the image\")\n\ndef analyse_image(image: Image) -&gt; AnalysisResult:\n    \"\"\"\n    Analyse the image and extract its\n    theme, description and objects.\n    \"\"\"\n    return chain()\n\nexample_image = Image.from_file(\"examples/assets/old_chinese_temple.jpg\")\n\nresult = analyse_image(example_image)\n\nprint(\"Theme:\", result.theme)\nprint(\"Description:\", result.description)\nfor obj in result.objects:\n    print(\"Found this object:\", obj)\n</code></pre>\n</code></pre> <p>Example Output</p> <pre><code>Theme: Traditional Japanese architecture and nature during rainfall\nDescription: The image depicts a serene rainy scene with traditional Japanese buildings. The warm glow of lights from the windows contrasts with the cool tones of the rain. A cherry blossom tree in bloom adds a splash of color to the otherwise muted scene. Stone lanterns and stepping stones create a path leading to the building, while hanging lanterns with a skull motif suggest a cultural or festive significance.\nFound this object: traditional Japanese building\nFound this object: cherry blossom tree\nFound this object: rain\nFound this object: stepping stones\nFound this object: stone lantern\nFound this object: hanging lanterns with skull motif\nFound this object: glowing windows\n</code></pre>"},{"location":"features/vision/#instructions","title":"Instructions","text":"<p>Step-by-step</p> <p>Oiur goal is the functionality is to analyze an image and extract its theme, a description, and a list of objects found within it.</p> <p>Necessary Imports</p> <pre><code>from funcchain import Image, chain, settings\nfrom pydantic import BaseModel, Field\n</code></pre> <p>Configure Settings</p> <p>The settings are configured to use a specific language model capable of image analysis and to enable console streaming for immediate output.</p> <pre><code>settings.llm = \"openai/gpt-4-vision-preview\"\nsettings.console_stream = True\n</code></pre> <p>Define the AnalysisResult Model</p> <p>The AnalysisResult class models the expected output of the image analysis, including the theme, description, and objects detected in the image.</p> <pre><code>class AnalysisResult(BaseModel):\n    theme: str = Field(description=\"The theme of the image\")\n    description: str = Field(description=\"A description of the image\")\n    objects: list[str] = Field(description=\"A list of objects found in the image\")\n</code></pre> <p>Implement the Image Analysis Function</p> <p>The analyse_image function is designed to take an Image object and use the chain function to process the image and return an AnalysisResult object for later usage (here printing).</p> <pre><code>def analyse_image(image: Image) -&gt; AnalysisResult:\n    return chain()\n</code></pre> <p>Execute the Analysis</p> <p>This block runs the image analysis on an example image and prints the results when the script is executed directly.</p> <pre><code>example_image = Image.from_file(\"examples/assets/old_chinese_temple.jpg\")\nresult = analyse_image(example_image)\nprint(\"Theme:\", result.theme)\nprint(\"Description:\", result.description)\nfor obj in result.objects:\n    print(\"Found this object:\", obj)\n</code></pre>"},{"location":"getting-started/config/","title":"Funcchain Configuration","text":""},{"location":"getting-started/config/#set-global-settings","title":"Set Global Settings","text":"<p>In every project you use funcchain in you can specify global settings. This is done by importing the <code>settings</code> object from the <code>funcchain</code> package.</p> <pre><code>from funcchain import settings\n</code></pre> <p>You can then change the settings like here:</p> <pre><code>settings.llm = \"openai/gpt-4-vision-preview\"\n</code></pre>"},{"location":"getting-started/config/#set-local-settings","title":"Set Local Settings","text":"<p>If you want to set local settings only applied to a specific funcchain function you can set them using the SettingsOverride class.</p> <pre><code>from funcchain import chain\nfrom funcchain.settings import SettingsOverride\n\ndef analyse_output(\n    goal: str\n    output: str,\n    settings: SettingsOverride = {},\n) -&gt; OutputAnalysis:\n    \"\"\"\n    Analyse the output and determine if the goal is reached.\n    \"\"\"\n    return chain(settings_override=settings)\n\nresult = analyse_output(\n    \"healthy outpout\",\n    \"Hello World!\",\n    settings_override={\"llm\": \"openai/gpt-4-vision-preview\"},\n)\n</code></pre> <p>The <code>settings_override</code> argument is a <code>SettingsOverride</code> object which is a dict-like object that can be used to override the global settings. You will get suggestions from your IDE on what settings you can override due to the type hints.</p>"},{"location":"getting-started/config/#settings-class-overview","title":"Settings Class Overview","text":"<p>The configuration settings for Funcchain are encapsulated within the <code>FuncchainSettings</code> class. This class inherits from Pydantic's <code>BaseSettings</code>.</p> <p><code>funcchain/backend/settings.py</code></p> <pre><code>class FuncchainSettings(BaseSettings):\n    ...\n</code></pre>"},{"location":"getting-started/config/#setting-descriptions","title":"Setting Descriptions","text":""},{"location":"getting-started/config/#general-settings","title":"General Settings","text":"<ul> <li> <p><code>debug: bool = True</code>   Enables or disables debug mode.</p> </li> <li> <p><code>llm: BaseChatModel | str = \"openai/gpt-3.5-turbo-1106\"</code>   Defines the language learning model to be used. It can be a type of <code>BaseChatModel</code> or <code>str</code> (model_name).   Checkout the MODELS.md file for a list and schema of supported models.</p> </li> <li> <p><code>console_stream: bool = False</code>   Enables or disables token streaming to the console.</p> </li> <li> <p><code>system_prompt: str = \"\"</code>   System prompt used as first message in the chat to instruct the model.</p> </li> <li> <p><code>retry_parse: int = 3</code>   Number of retries for auto fixing pydantic validation errors.</p> </li> <li> <p><code>retry_parse_sleep: float = 0.1</code>   Sleep time between retries.</p> </li> </ul>"},{"location":"getting-started/config/#model-keyword-arguments","title":"Model Keyword Arguments","text":"<ul> <li> <p><code>verbose: bool = False</code>   Enables or disables verbose logging for the model.</p> </li> <li> <p><code>streaming: bool = False</code>   Enables or disables streaming for the model.</p> </li> <li> <p><code>max_tokens: int = 2048</code>   Specifies the maximum number of output tokens for chat models.</p> </li> <li> <p><code>temperature: float = 0.1</code>   Controls the randomness in the model's output.</p> </li> </ul>"},{"location":"getting-started/config/#llamacpp-keyword-arguments","title":"LlamaCPP Keyword Arguments","text":"<ul> <li> <p><code>context_lenght: int = 8196</code>   Specifies the context length for the LlamaCPP model.</p> </li> <li> <p><code>n_gpu_layers: int = 42</code>   Specifies the number of GPU layers for the LlamaCPP model.   Choose 0 for CPU only.</p> </li> <li> <p><code>keep_loaded: bool = False</code>   Determines whether to keep the LlamaCPP model loaded in memory.</p> </li> <li> <p><code>local_models_path: str = \"./.models\"</code>   Specifies the local path for storing models.</p> </li> </ul>"},{"location":"getting-started/demos/","title":"Demos","text":""},{"location":"getting-started/demos/#demos","title":"Demos","text":""},{"location":"getting-started/demos/#simple-structured-output","title":"Simple Structured Output","text":"<pre><code>from funcchain import chain\nfrom pydantic import BaseModel\n\n# define your output shape\nclass Recipe(BaseModel):\n    ingredients: list[str]\n    instructions: list[str]\n    duration: int\n\n# write prompts utilising all native python features\ndef generate_recipe(topic: str) -&gt; Recipe:\n    \"\"\"\n    Generate a recipe for a given topic.\n    \"\"\"\n    return chain() # &lt;- this is doing all the magic\n\n# generate llm response\nrecipe = generate_recipe(\"christmas dinner\")\n\n# recipe is automatically converted as pydantic model\nprint(recipe.ingredients)\n</code></pre> <p>Step-by-step</p> <pre><code>class Recipe(BaseModel):\n    ingredients: list[str]\n    instructions: list[str]\n    duration: int\n</code></pre> <p>A Recipe class is defined, inheriting from BaseModel (pydantic library). This class specifies the structure of the output data, which you can customize. In the example it includes a list of ingredients, a list of instructions, and an integer representing the duration</p> <p><pre><code>def generate_recipe(topic: str) -&gt; Recipe:\n    \"\"\"\n    Generate a recipe for a given topic.\n    \"\"\"\n    return chain()\n</code></pre> In this example the <code>generate_recipe</code> function takes a topic string and returns a <code>Recipe</code> instance for that topic.</p>"},{"location":"getting-started/demos/#understanding-chain-functionality","title":"Understanding chain() Functionality","text":"<p>Chain() is the backend magic of funcchain. Behind the szenes it creates the llm function from the function signature and docstring. Meaning it will turn your function into usable LLM input.</p> <p>The <code>chain()</code> function is the core component of funcchain. It takes the docstring, input arguments and return type of the function and compiles everything into a langchain runnable . It then executes the prompt with your input arguments if you call the function and returns the parsed result.</p>"},{"location":"getting-started/demos/#print-your-response","title":"Print your response","text":"<pre><code>recipe = generate_recipe(\"christmas dinner\")\n\nprint(recipe.ingredients)\n</code></pre>"},{"location":"getting-started/demos/#demo","title":"Demo","text":"<pre><code>$ print(generate_recipe(\"christmas dinner\").ingredients\n\n['turkey', 'potatoes', 'carrots', 'brussels sprouts', 'cranberry sauce', 'gravy','butter', 'salt', 'pepper', 'rosemary']\n</code></pre>"},{"location":"getting-started/demos/#complex-structured-output","title":"Complex Structured Output","text":"<p>(full code)</p> <p>Step-by-step</p> <p>Nececary Imports</p> <pre><code>from pydantic import BaseModel, Field\nfrom funcchain import chain\n</code></pre> <p>Data Structures and Model Definitions</p> <pre><code># define nested models\nclass Item(BaseModel):\n    name: str = Field(description=\"Name of the item\")\n    description: str = Field(description=\"Description of the item\")\n    keywords: list[str] = Field(description=\"Keywords for the item\")\n\nclass ShoppingList(BaseModel):\n    items: list[Item]\n    store: str = Field(description=\"The store to buy the items from\")\n\nclass TodoList(BaseModel):\n    todos: list[Item]\n    urgency: int = Field(description=\"The urgency of all tasks (1-10)\")\n</code></pre> <p>In this example, we create a more complex data structure with nested models. The Item model defines the attributes of a single item, such as its name, description, and keywords. ShoppingList and TodoList models define the attributes of a shopping list and a todo list, utilizing the Item model as a nested model.</p> <p>You can define new Pydantic models or extend existing ones by adding additional fields or methods. The general approach is to identify the data attributes relevant to your application and create corresponding model classes with these attributes.</p> <p>The Field descriptions serve as prompts for the language model to understand the data structure. Additionally you can include a docstring for each model class to provide further information to the LLM.</p> <p>Important</p> <p>Everything including class names, argument names, doc string and field descriptions are part of the prompt and can be optimised using prompting techniques.</p> <p>Union types</p> <pre><code># support for union types\ndef extract_list(user_input: str) -&gt; TodoList | ShoppingList:\n    \"\"\"\n    The user input is either a shopping List or a todo list.\n    \"\"\"\n    return chain()\n</code></pre> <p>The extract_list function uses the chain function to analyze user input and return a structured list: In the example: - Union Types: It can return either a TodoList or a ShoppingList, depending on the input. - Usage of chain: chain simplifies the process, deciding the type of list to return.</p> <p>For your application this is going to serve as a router to route between your previously defined models.</p> <p>Get a list from the user (here as \"lst\")</p> <pre><code>lst = extract_list(\"Complete project report, Prepare for meeting, Respond to emails\")\n</code></pre> <p>Define your custom handlers</p> <p>And now its time to define what happens with the result. You can then use the lst (list) variable to access the attributes of the list. It utilizes pattern matching to determine the type of list and print the corresponding output.</p> <pre><code>match lst:\n    case ShoppingList(items=items, store=store):\n        print(\"Shopping List: \")\n        for item in items:\n            print(f\"{item.name}: {item.description}\")\n        print(f\"You need to go to: {store}\")\n\n    case TodoList(todos=todos, urgency=urgency):\n        print(\"Todo List: \")\n        for item in todos:\n            print(f\"{item.name}: {item.description}\")\n        print(f\"Urgency: {urgency}\")\n</code></pre> <pre><code>$ extract_list(\"Complete project report, Prepare for meeting, Respond to emails\")\n\nTodo List:\nComplete project report: Finish the project report and submit it\nPrepare for meeting: Gather all necessary materials and information for the meeting\nRespond to emails: Reply to all pending emails\nUrgency: 8\n</code></pre>"},{"location":"getting-started/demos/#vision-models","title":"Vision Models","text":"<pre><code>from PIL import Image\nfrom pydantic import BaseModel, Field\nfrom funcchain import chain, settings\n\n# set global llm using model identifiers (see MODELS.md)\nsettings.llm = \"openai/gpt-4-vision-preview\"\n\n# everything defined is part of the prompt\nclass AnalysisResult(BaseModel):\n    \"\"\"The result of an image analysis.\"\"\"\n\n    theme: str = Field(description=\"The theme of the image\")\n    description: str = Field(description=\"A description of the image\")\n    objects: list[str] = Field(description=\"A list of objects found in the image\")\n\n# easy use of images as input with structured output\ndef analyse_image(image: Image.Image) -&gt; AnalysisResult:\n    \"\"\"\n    Analyse the image and extract its\n    theme, description and objects.\n    \"\"\"\n    return chain()\n\nresult = analyse_image(Image.open(\"examples/assets/old_chinese_temple.jpg\"))\n\nprint(\"Theme:\", result.theme)\nprint(\"Description:\", result.description)\nfor obj in result.objects:\n    print(\"Found this object:\", obj)\n</code></pre> <p>Step-by-step</p> <p>Nececary Imports</p> <pre><code>from PIL import Image\nfrom pydantic import BaseModel, Field\nfrom funcchain import chain, settings\n</code></pre> <p>Define Model</p> <p>set global llm using model identifiers see MODELS.md</p> <pre><code>settings.llm = \"openai/gpt-4-vision-preview\"\n</code></pre> <p>Funcchains modularity allows for all kinds of models including local models</p> <p>Analize Image</p> <p>Get structured output from an image in our example <code>theme</code>, <code>description</code> and <code>objects</code></p> <p><pre><code># everything defined is part of the prompt\nclass AnalysisResult(BaseModel):\n    \"\"\"The result of an image analysis.\"\"\"\n\n    theme: str = Field(description=\"The theme of the image\")\n    description: str = Field(description=\"A description of the image\")\n    objects: list[str] = Field(description=\"A list of objects found in the image\")\n</code></pre> Adjsut the fields as needed. Play around with the example, feel free to experiment. You can customize the analysis by modifying the fields of the <code>AnalysisResult</code> model.</p> <p>Function to start the analysis</p> <pre><code># easy use of images as input with structured output\ndef analyse_image(image: Image.Image) -&gt; AnalysisResult:\n    \"\"\"\n    Analyse the image and extract its\n    theme, description and objects.\n    \"\"\"\n    return chain()\n</code></pre> <p>Chain() will handle the image input. We here define again the fields from before <code>theme</code>, <code>description</code> and <code>objects</code></p> <p>give an image as input <code>image: Image.Image</code></p> <p>Its important that the fields defined earlier are mentioned here with the prompt <code>Analyse the image and extract its</code>...</p> <pre><code>result = analyse_image(\n    Image.from_file(\"examples/assets/old_chinese_temple.jpg\")\n)\n\nprint(\"Theme:\", result.theme)\nprint(\"Description:\", result.description)\nfor obj in result.objects:\n    print(\"Found this object:\", obj)\n\n$ ..................\n\nTheme: Traditional Japanese architecture and nature during rainfall\nDescription: The image depicts a serene rainy scene at night in a traditional Japanese setting. A two-story wooden building with glowing green lanterns is the focal point, surrounded by a cobblestone path, a blooming pink cherry blossom tree, and a stone lantern partially obscured by the rain. The atmosphere is tranquil and slightly mysterious.\nFound this object: building\nFound this object: green lanterns\nFound this object: cherry blossom tree\nFound this object: rain\nFound this object: cobblestone path\nFound this object: stone lantern\nFound this object: wooden structure\n</code></pre>"},{"location":"getting-started/demos/#seamless-local-model-support","title":"Seamless local model support","text":"<p>Yes you can use funcchain without internet connection. Start heating up your device.</p> <pre><code>from pydantic import BaseModel, Field\nfrom funcchain import chain, settings\n\n# auto-download the model from huggingface\nsettings.llm = \"ollama/openchat\"\n\nclass SentimentAnalysis(BaseModel):\n    analysis: str\n    sentiment: bool = Field(description=\"True for Happy, False for Sad\")\n\ndef analyze(text: str) -&gt; SentimentAnalysis:\n    \"\"\"\n    Determines the sentiment of the text.\n    \"\"\"\n    return chain()\n\n# generates using the local model\npoem = analyze(\"I really like when my dog does a trick!\")\n\n# promised structured output (for local models!)\nprint(poem.analysis)\n</code></pre> <p>Step-by-step</p> <p>Nececary Imports</p> <pre><code>from pydantic import BaseModel, Field\nfrom funcchain import chain, settings\n</code></pre> <p>Choose and enjoy</p> <pre><code>settings.llm = \"llamacpp/openchat-3.5-0106:Q3_K_M\"\n</code></pre> <p>Structured output definition</p> <p>With an input <code>str</code> a description can be added to return a boolean <code>true</code> or <code>false</code></p> <pre><code>class SentimentAnalysis(BaseModel):\n    analysis: str\n    sentiment: bool = Field(description=\"True for Happy, False for Sad\")\n</code></pre> <p>Experiment yourself by adding different descriptions for the true and false case.</p> <p>Use <code>chain()</code> to analize</p> <p>Defines with natural language the analysis</p> <pre><code>def analyze(text: str) -&gt; SentimentAnalysis:\n    \"\"\"\n    Determines the sentiment of the text.\n    \"\"\"\n    return chain()\n</code></pre> <p>For your own usage adjust the str. Be precise and reference your classes again.</p> <p>Generate and print the output <pre><code>poem = analyze(\"I really like when my dog does a trick!\")\nprint(poem.analysis)\n</code></pre></p> <p>Useful</p> <p>For seeing whats going on inside the LLM you should try the Langsmith integration: Add those lines to .env and funcchain will use langsmith tracing.</p> <pre><code>LANGCHAIN_TRACING_V2=true\nLANGCHAIN_API_KEY=\"ls__api_key\"\nLANGCHAIN_PROJECT=\"PROJECT_NAME\"\n</code></pre> <p>Langsmith is used to understand what happens under the hood of your LLM generations. When multiple LLM calls are used for an output they can be logged for debugging.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installation","title":"Installation","text":"<pre><code>$ pip install funcchain\n---&gt; 100%\n</code></pre> <p>For additional features you can also install:</p> <ul> <li><code>funcchain</code> (langchain_core + openai)</li> <li><code>funcchain[ollama]</code> (you need to install this ollama fork for grammar support)</li> <li><code>funcchain[llamacpp]</code> (using llama-cpp-python)</li> <li><code>funcchain[pillow]</code> (for vision model features)</li> <li><code>funcchain[all]</code> (includes everything)</li> </ul> <p>To enter this in your terminal you need to write it like this: <code>pip install \"funcchain[all]\"</code></p>"},{"location":"getting-started/installation/#environment","title":"Environment","text":"<p>Make sure to have an OpenAI API key in your environment variables. For example,</p> <pre><code>export OPENAI_API_KEY=\"sk-rnUPxirSQ4bmz2He4qyaiKShdXJcsOsTg\"\n</code></pre> <p>But you can also create a <code>.env</code> file in your current working directory and include the key there. The dot env file will load automatically.</p>"},{"location":"getting-started/models/","title":"Supported Models","text":""},{"location":"getting-started/models/#langchain-chat-models","title":"LangChain Chat Models","text":"<p>You can set the <code>settings.llm</code> with any LangChain ChatModel.</p> <pre><code>from funcchain import settings\nfrom langchain_openai.chat_models import AzureChatOpenAI\n\nsettings.llm = AzureChatOpenAI(...)\n</code></pre>"},{"location":"getting-started/models/#string-model-identifiers","title":"String Model Identifiers","text":"<p>You can also set the <code>settings.llm</code> with a string identifier of a ChatModel including local models.</p> <pre><code>from funcchain import settings\n\nsettings.llm = \"llamacpp/openchat-3.5-1210\"\n\n# ...\n</code></pre>"},{"location":"getting-started/models/#schema","title":"Schema","text":"<p><code>&lt;provider&gt;/&lt;model_name&gt;:&lt;optional_label&gt;</code></p>"},{"location":"getting-started/models/#providers","title":"Providers","text":"<ul> <li><code>openai</code>: OpenAI Chat Models</li> <li><code>llamacpp</code>: Run local models directly using llamacpp (alias: <code>thebloke</code>, <code>gguf</code>)</li> <li><code>ollama</code>: Run local models through Ollama (wrapper for llamacpp)</li> <li><code>azure</code>: Azure Chat Models</li> <li><code>anthropic</code>: Anthropic Chat Models</li> <li><code>google</code>: Google Chat Models</li> </ul>"},{"location":"getting-started/models/#examples","title":"Examples","text":"<ul> <li><code>openai/gpt-3.5-turbo</code>: ChatGPT Classic</li> <li><code>openai/gpt-4-1106-preview</code>: GPT-4-Turbo</li> <li><code>ollama/openchat</code>: OpenChat3.5-1210</li> <li><code>ollama/openhermes2.5-mistral</code>: OpenHermes 2.5</li> <li><code>llamacpp/openchat-3.5-1210</code>: OpenChat3.5-1210</li> <li><code>TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF</code>: alias for <code>llamacpp/...</code></li> <li><code>TheBloke/openchat-3.5-0106-GGUF:Q3_K_L</code>: with Q label</li> </ul>"},{"location":"getting-started/models/#additional-notes","title":"additional notes","text":"<p>Checkout the file <code>src/funcchain/model/defaults.py</code> for the code that parses the string identifier. Feel free to create a PR to add more models to the defaults. Or tell me how wrong I am and create a better system.</p>"},{"location":"getting-started/usage/","title":"Usage","text":"<p>To write your cognitive architectures with the funcchain syntax you need to import the <code>chain</code> function from the <code>funcchain</code> package.</p> <pre><code>from funcchain import chain\n</code></pre> <p>This chain function it the core component of funcchain. It takes the docstring, input arguments and return type of the function and compiles everything into a langchain prompt. It then executes the prompt with your input arguments if you call the function and returns the parsed result.</p> <pre><code>def hello(lang1: str, lang2: str, lang3: str) -&gt; list[str]:\n    \"\"\"\n    Say hello in these 3 languages.\n    \"\"\"\n    return chain()\n\nhello(\"German\", \"French\", \"Spanish\")\n</code></pre> <p>The underlying chat in the background will look like this:</p> <pre><code>&lt;HumanMessage&gt;\nLANG1: German\nLANG2: French\nLANG3: Spanish\n\nSay hello in these 3 languages.\n&lt;/HumanMessage&gt;\n\n&lt;AIMessage&gt;\n{\n    \"value\": [\"Hallo\", \"Bonjour\", \"Hola\"]\n}\n&lt;/AIMessage&gt;\n</code></pre> <p>Funcchain is handling all the redundant and complicated structuring of your prompts so you can focus on the important parts of your code.</p> <p>All input arguments are automatically added to the prompt so the model has context about what you insert. The return type is used to force the model using a json-schema to always return a json object in the desired shape.</p>"}]}